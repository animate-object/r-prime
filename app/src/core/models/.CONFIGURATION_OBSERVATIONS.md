###Observations concerning the NN configurations under test.
As stated this document contains some informal observations on various training runs of networks in r-prime.
These observations are not especially scientific and serve to prioritize possible training configurations
from the space of possible NN parameters. The models described correspond to classes defined in this same
directory. Note that this document was created simply to avoid pasting observations right into the doc strings
of those definitions.

####Log 1 -- 4/9 - 4/10 
#####Some findings around hidden layers -- depth and width.
The first and perhaps most obvious space to tinker with has been with regard to the 'dimensions' of
the hidden layers of the neural network. There is of course an immense amount of literature on the subject,
but often times experimentation is a more natural vector for initial human learning. 
######General observations
Before describing model specifics, here are some general observations gleaned from the first batch of models examined:

1. Network depth is temporally expensive
    * 'deeper' networks with more hidden layers seem to take significantly longer to train. 
    This relationship is at least linear
2. Network width is memory intensive
    * When given identical counts of hidden layers, wider networks (more neurons per hidden layer) have more
    potential to cause out of memory exceptions.
3. In combination, from a computational perspective the previous observations suggest that we want to balance
width and depth
4. Wider networks with fewer layers seem to achieve better loss at a faster rate than deeper, narrower networks
in the same training time.
5. Without getting too specific, it seems that over training time, the rate at which our models *learn*, seems to 
decrease as training time increases. To that end, there may exist mathematical 'limits' at which the rate of 
new learning approaches zero, and additional epochs of training time are effectively wasted.

#####Models
######Basic LSTM Rnn
Using the long term short term NN model, this configuration was our jumping board for character RNNs, lifted entirely
from a tflearn demo using shakespeare data. This NN consists of 3 hidden layers of 512 neurons each and what we assume
are relatively 'normal' *other* parameters (loss function, optimization algorithm, learning rate). We viewing it as a
statistical control.

Running this model for 100 epochs on our training data produced reasonable results. The model generated an impressive
number of English words and mimicked some phrasal logic.

######Deep and Narrow model (HotdogRnn)
Consisted of 6 LSTM layers of 256 neurons each and one of 512. This model produced poor results along 100 epochs of training. It did
not appear to pick up on regular word structure and exhibited highly irregular spacing between words. It is interesting
that 256 neurons does not seem to be enough to learn the relationships between one hundred characters in equivalent
training time compared to the basic model. The results from this model were our first inkling that deeper does not mean
'better'. Although of course, the width of our layers could also be the culprit

######Deep model (JumboDogRnn)
Consisted of 7 LSTM layers of 512 (our control number) neurons each. This model took prohibitively long to train and
was abandoned mid session. Casual observation of the loss function showed that over the current 30 epochs, the output
of our loss function did not seem to be improving on pace with the basic model -- achieving relatively unsatisfactory
results around 3.0

######Wide model (HamburgerRnn)
Consists of 3 hidden layers of 1024 neurons each and one of 512. Dissatisfied with the results of our 'deeper' networks,
we're examining a wider model. This model seems to be exhibiting significantly better performance with regards to loss,
achieving sub 2.0 loss around 15 epochs (~1.5 at the time of writing mid epoch 26). This significant improvement will
guide us toward more experimentation with wider, shallower models. Additionally, from here on out, we will refrain from
training any model that doesn't seem especially promising for 100 epochs. That amount of time is simply impractical when
successive experiments seem to produce alternatively unsatisfactory results. 

Interestingly, after epoch 10 or so, the rate of learning seems to have slowed by an order of magnitude. May be logarithmic.

At its conclusion the Wide model produced the best results we have hitherto seen. As one group member commented:
**"Raptimus Prime can RAP!"**

######Wide, Shallow Model (PancakeRnn)
Consists of 1 hidden layer of 1024 neurons and one of 512. It will be interesting to see if this reduction in hidden
layers significantly impacts the loss limit of this model. So far results seem good with sub 3 loss within the first
epoch, steadily dropping into epoch 2. Will continue to monitor. Corroborating with previous results, the decrease in
layer size has made computing this net comparatively efficient.

Seeing sub 2.0 loss results mid epoch 6. Continuing to monitor.

#####Potential future layer configurations.
A super wide, shallow model -- one layer of 2048 neurons and one of 512

As we progress, we will be experimenting with other parameters, such as optimization and loss functions as well as
learning rate.

####Log 1 -- 4/20 - 4/23
We've been intermittently playing with new configurations. Over the next couple of days we'll be preparing the NNs
that we package with the application. These are documented roughly in order.

#####Revised wide model trained against Kendrick Lamar's discography for 40 epochs.
This revised hamburger RNN has 2 hidden layers of 1024 neurons. It learned very quickly.

An interesting experiment. A couple things to note here. Kendrick's discography is notably smaller than the previous
training data -- roughly half the output of Nas who we've been working with. This may not be enough data to sufficiently
convey the 'style' of an artist. Further more, 40 epochs proved to much. With final loss function values around the 
0.1s, this function had a tendency toward copy right infringement, reciting lines verbatim from Kendrick's discography. 
In particular this model seemed to latch onto chorus lines and repeat them endlessly. We will try again with the same 
network configuration but training for a shorter duration

